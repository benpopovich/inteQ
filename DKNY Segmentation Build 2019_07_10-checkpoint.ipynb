{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyodbc\n",
    "import pandas as pd\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "from datetime import timedelta\n",
    "from datetime import datetime\n",
    "from dateutil import parser as dtparser\n",
    "def dt_form(x):\n",
    "    return dt.date.isoformat(x)\n",
    "import csv\n",
    "#with open('Q:\\\\Confidential\\\\DKNY\\\\Analytics\\\\2019\\\\201905 - Segmentation\\\\Eric\\\\Eric_csv_test.csv', 'w', newline='') as f:\n",
    "    #writer = csv.writer(f)\n",
    "\n",
    "\n",
    "\n",
    "#path = \"Q:\\\\Confidential\\\\DKNY\\Analytics\\\\2019\\\\201905 - Segmentation\\\\Eric\\\\\"\n",
    "period_end = dt.date(2019,5,31)\n",
    "period_st = dt.date(2017,6,1)\n",
    "period_12mo = dt.date(2018,5,31)\n",
    "one_day = timedelta(days=1)\n",
    "sweepkey = 25\n",
    "\n",
    "#  Outfile location to be determined.\n",
    " #outfile = \"Q:\\Confidential\\Analytics\\BI\\DW Loads\\DKNY\\Segment Files\\segments_may2019.txt\";\n",
    "\n",
    "#filename = may19segscore\n",
    "\n",
    "#Code to connect to data warehouse 1. driver = type of server. \n",
    "# server = sql server's name and database = which database on the sql server you want to access.\n",
    "# trusted connection = yes just means you won't have a security error thrown.\n",
    "connection = pyodbc.connect(driver = '{SQL SERVER}', \n",
    "                            Server = 'shcd-chi-dw1',\n",
    "                            Database = 'DW_DKNY',\n",
    "                            trusted_connection = 'yes')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-06-01\n",
      "customerkey    2153963\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# SQL query to create a member table\n",
    "mem = \"\"\"  \\\n",
    "select customerkey, enrolldate, store1distance\n",
    "from fact_member \n",
    "where consolidatedcustomerkey is null\n",
    "and convert(date,enrolldate) < \"\"\" + \"'\"+ dt_form(period_end + one_day) + \"'\" + \"and countrycode not in ('GB','IE') \"\n",
    "\n",
    "print(period_end + one_day)\n",
    "# Creation of member table\n",
    "mem_table = pd.read_sql_query(mem, connection)\n",
    "\n",
    "print(mem_table.agg({'customerkey':'count'}))\n",
    "\n",
    "# SQL query to create a transaction table\n",
    "trans = \"\"\"  \\\n",
    "select transactionkey, customerkey, transactiondate, netsalesamount\n",
    "from fact_transaction \n",
    "where customerkey is not null\n",
    "and transactiondate >= \"\"\" + \"'\" + dt_form(period_st) + \"' and transactiondate <= '\" + dt_form(period_end) + \"'\"\n",
    "\n",
    "# Creation of a transaction table\n",
    "trans_table = pd.read_sql_query(trans,connection)\n",
    "\n",
    "\n",
    "# SQL query to create an active 12 month table\n",
    "active12mo = \"\"\"  \\\n",
    "select customerkey, count(*) as trans_ct12mo, sum(netsalesamount) as to_netsales12mo\n",
    "from fact_transaction\n",
    "where convert(date,transactiondate) between \\\n",
    "\"\"\" + \"'\"+ dt_form(period_12mo + one_day) + \"' and '\" + dt_form(period_end) + \"'\" + \"\"\" \\\n",
    "group by customerkey\n",
    "order by customerkey;\n",
    "\"\"\"\n",
    "\n",
    "# Creation of a 12 month active member table\n",
    "active12mo_table = pd.read_sql_query(active12mo,connection)\n",
    "\n",
    "# SQL query to create a transaction detail table\n",
    "transdet = \"\"\"  \\\n",
    "select transdetailkey, transactionkey, customerkey, productkey, \n",
    "quantity, netsalesamount, ic_discountamount, transactiondate\n",
    "from fact_transdetail\n",
    "where customerkey is not null\n",
    "and '\"\"\" + dt_form(period_st) + \"' <= convert(date,transactiondate) and convert(date,transactiondate) <= '\"+ \\\n",
    "dt_form(period_end)+\"'\"\n",
    "\n",
    "# Creation of a transaction detail table\n",
    "transdet_table = pd.read_sql_query(transdet,connection)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "productkey                int64\n",
      "categoryname             object\n",
      "divisionname             object\n",
      "departmentname           object\n",
      "DK WOMENS APPAREL_FL      int32\n",
      "DK MENS APPAREL_FL        int32\n",
      "HANDBAGS_FL               int32\n",
      "SMALL LEATHERGOODS_FL     int32\n",
      "Womens Outerwear_FL       int32\n",
      "HATS / MISC_FL            int32\n",
      "Mens Outerwear_FL         int32\n",
      "FOOTWEAR_FL               int32\n",
      "APPAREL_FL                int32\n",
      "Outerwear_FL              int32\n",
      "WOMENS_FL                 int32\n",
      "MENS_FL                   int32\n",
      "ECOMMERCE_FL              int32\n",
      "dtype: object transdetailkey                int64\n",
      "transactionkey                int64\n",
      "customerkey                   int64\n",
      "productkey                    int64\n",
      "quantity                      int64\n",
      "netsalesamount              float64\n",
      "ic_discountamount           float64\n",
      "transactiondate      datetime64[ns]\n",
      "activity_24mo_FL              int32\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Creation of a temporary transaction detail table named trans1\n",
    "# We are also adding a 24 month activity flag variable to the temp. table.\n",
    "# We initially set it to 0, then update the value to 1 if the transaction date is before 12 month's\n",
    "#from the end of the current month.\n",
    "trans1det = transdet_table.copy()\n",
    "\n",
    "#trans1det = trans1det.insert(loc=len(trans1det.columns),column='activity_24mo_FL', value=0)\n",
    "trans1det['activity_24mo_FL'] = np.where(trans1det['transactiondate'] < pd.to_datetime(period_12mo + one_day),1,0)\n",
    "\n",
    "\n",
    "# SQL query to create a product table\n",
    "prod = \"\"\"  \\\n",
    "select productkey, categoryname, divisionname, departmentname\n",
    "from dim_product;\"\"\"\n",
    "\n",
    "# Creation of a product table\n",
    "prod_table = pd.read_sql_query(prod,connection,coerce_float = False)\n",
    "\n",
    "\n",
    "#### Adding Department Name product category and ecomm flags to product table\n",
    "depts = [\"DK WOMENS APPAREL\", \"DK MENS APPAREL\", 'HANDBAGS', 'SMALL LEATHERGOODS', 'Womens Outerwear', 'HATS / MISC', \n",
    "        'Mens Outerwear', 'FOOTWEAR']\n",
    "cats = ['APPAREL', 'Outerwear', 'WOMENS', 'MENS', 'ECOMMERCE']\n",
    "\n",
    "\n",
    "# Creating and assinging department flag columns\n",
    "for x in depts:\n",
    "    prod_table[x+\"_FL\"] = np.where(prod_table['departmentname'] == x, 1, 0)\n",
    "\n",
    "# Creating category flag coloumns\n",
    "for x in cats:\n",
    "    prod_table.insert(loc=len(prod_table.columns),column=x+\"_FL\", value = 0)\n",
    "\n",
    "    \n",
    "    \n",
    "# Creating and Assigning Category Flag Values by using the max function across a collection\n",
    "# of columns to assign a value to each row.\n",
    "prod_table[ 'APPAREL_FL'] = prod_table[['DK WOMENS APPAREL_FL', 'DK MENS APPAREL_FL']].max(axis = 1)\n",
    "prod_table[ 'Outerwear_FL'] = prod_table[['Womens Outerwear_FL', 'Mens Outerwear_FL']].max(axis = 1)\n",
    "prod_table[ 'WOMENS_FL'] = prod_table[['Womens Outerwear_FL', 'HANDBAGS_FL', 'DK WOMENS APPAREL_FL']].max(axis = 1)\n",
    "prod_table[ 'MENS_FL'] = prod_table[['DK MENS APPAREL_FL', 'Mens Outerwear_FL']].max(axis = 1)\n",
    "prod_table['ECOMMERCE_FL'] = np.where(prod_table['productkey'] == 47455, 1,0)\n",
    "\n",
    "\n",
    "\n",
    "trans1det['productkey'] = trans1det['productkey'].fillna(-1)\n",
    "trans1det['productkey'] = trans1det['productkey'].astype('int64')\n",
    "print(prod_table.dtypes, trans1det.dtypes)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [transdetailkey, transactionkey, customerkey, productkey, quantity, netsalesamount, ic_discountamount, transactiondate, activity_24mo_FL, categoryname, divisionname, departmentname, DK WOMENS APPAREL_FL, DK MENS APPAREL_FL, HANDBAGS_FL, SMALL LEATHERGOODS_FL, Womens Outerwear_FL, HATS / MISC_FL, Mens Outerwear_FL, FOOTWEAR_FL, APPAREL_FL, Outerwear_FL, WOMENS_FL, MENS_FL, ECOMMERCE_FL]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 25 columns]\n",
      "active 12 month:  customerkey    170392\n",
      "dtype: int64\n",
      "trans_det:  transactionkey    1177150\n",
      "dtype: int64\n",
      "trans1det:  transactionkey    1177150\n",
      "dtype: int64\n",
      "prod_table:  productkey    59515\n",
      "dtype: int64\n",
      "trans_det_prod:  transactionkey    709385\n",
      "dtype: int64\n",
      "trans_det_prod_roll:  transactionkey    255676\n",
      "dtype: int64\n",
      "trans_table:  transactionkey    421553\n",
      "dtype: int64\n",
      "trans_product:  customerkey    255649\n",
      "dtype: int64\n",
      "Index(['transactionkey', 'customerkey', 'transactiondate', 'netsalesamount',\n",
      "       'to_netsalesamount', 'ic_discountamount', 'quantity',\n",
      "       'DK WOMENS APPAREL_FL', 'DK MENS APPAREL_FL', 'HANDBAGS_FL',\n",
      "       'SMALL LEATHERGOODS_FL', 'Womens Outerwear_FL', 'HATS / MISC_FL',\n",
      "       'Mens Outerwear_FL', 'FOOTWEAR_FL', 'APPAREL_FL', 'Outerwear_FL',\n",
      "       'WOMENS_FL', 'MENS_FL', 'ECOMMERCE_FL', 'activity_24mo_FL'],\n",
      "      dtype='object')\n",
      "trans_roll:  customerkey    170334\n",
      "dtype: int64\n",
      "mem_trans:  customerkey    170284\n",
      "dtype: int64\n",
      "mem_table:  customerkey    2153963\n",
      "dtype: int64\n",
      "customerkey                       int64\n",
      "enrolldate               datetime64[ns]\n",
      "store1distance                  float64\n",
      "netsalesamount                  float64\n",
      "first_visit              datetime64[ns]\n",
      "last_visit               datetime64[ns]\n",
      "DK WOMENS APPAREL_FL               int8\n",
      "DK MENS APPAREL_FL                 int8\n",
      "HANDBAGS_FL                        int8\n",
      "SMALL LEATHERGOODS_FL              int8\n",
      "Womens Outerwear_FL                int8\n",
      "HATS / MISC_FL                     int8\n",
      "Mens Outerwear_FL                  int8\n",
      "FOOTWEAR_FL                        int8\n",
      "APPAREL_FL                         int8\n",
      "Outerwear_FL                       int8\n",
      "WOMENS_FL                          int8\n",
      "MENS_FL                            int8\n",
      "ECOMMERCE_FL                       int8\n",
      "activity_24mo_FL                   int8\n",
      "dtype: object\n",
      "customerkey    170231\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Left joining the transaction detail and product tables on product key\n",
    "trans_det_prod = pd.merge(trans1det, prod_table, how='left', left_on = 'productkey', right_on = 'productkey')\n",
    "\n",
    "# The code above leaves a lot of NaN values in the table.\n",
    "# For this reason, it has forced the datatypes of all of the flags to be floats.\n",
    "# The line of code below is swtiching all of the NaN values to 0.\n",
    "# This may not be advisable in every situation, but it is fine here since there\n",
    "# is no productkey with a value of 0 and after I do this, I can change\n",
    "# all of the flag varialbes to have an integer datatype.\n",
    "##############################\n",
    "\n",
    "\n",
    "# The line below turns all of the NaN values in to 0.\n",
    "trans_det_prod = trans_det_prod.fillna(0)\n",
    "\n",
    "\n",
    "\n",
    "# The for loop below turns the datatypes for all of the flag\n",
    "# variables into integer types. This is not necessary, but it might\n",
    "# save some time with formatting later and is my personal preference.\n",
    "# I am also creating a dictionary of column names and max aggregate functions\n",
    "# to use in the .agg function below for aggregating column information.\n",
    "lis = []\n",
    "for x in prod_table.columns:\n",
    "    if \"_FL\" in x:\n",
    "        trans_det_prod[x] = trans_det_prod[x].astype('int8')\n",
    "        lis.append((x,'max'))\n",
    "#################################        \n",
    "\n",
    "\n",
    "# The code in the line below creates a new table that displays\n",
    "# whether or not a customer has shopped in the last 24 months by customerkey\n",
    "# Python's Pandas package has a lot of SQL like functio built into it.\n",
    "# We should expect this since it is a database manipulation package.\n",
    "trans_24mo = trans_det_prod.groupby('customerkey', as_index=False).agg({'activity_24mo_FL': 'max'})\n",
    "##################\n",
    "\n",
    "trans_det_prod = trans_det_prod.loc[trans_det_prod['transactiondate'] > '2018-05-31']\n",
    "print(trans_det_prod.query(\"transactiondate < '2018-06-01'\"))\n",
    "\n",
    "\n",
    "\n",
    "# We are making a dictionary with keys the columns we want to aggregate from\n",
    "# and key values the aggregating functions (in this case just sums and a lot of max...s).\n",
    "# Then we are creating an aggregating table trans_det_prod_roll.\n",
    "# We are taking the max of all of the flag variables and suming the columns\n",
    "# 'netsalesamount', 'ic_discountamount, and 'quantity'.\n",
    "dic = {'netsalesamount':'sum','ic_discountamount': 'sum', 'quantity': 'sum'}\n",
    "dic.update(dict(lis))\n",
    "trans_det_prod_roll = trans_det_prod.groupby('transactionkey', as_index=False).agg(dic)\n",
    "#trans_det_prod_roll['transactionkey'] = trans_det_prod_roll['transactionkey'].astype(int)\n",
    "\n",
    "\n",
    "\n",
    "#trans_det_prod_roll\n",
    "\n",
    "# The bit of code below is to change the column name, 'netsalesamount' to 'to_netsalesamount'. \n",
    "# If we do not do this now, then when we merge the tables trans_table and _trans_det_prod_roll\n",
    "# below, we will get two columns labelled 'netsalesamount_x' and 'netsalesamount_y'.\n",
    "# while this is not inherently an issue and could be fixed by just dropping one of the \n",
    "# 'netsalesamount' columns, it follows the SAS code and demonstrates that to change\n",
    "# column names for a dataframe, we can feed in a dictionary to the .replace() function \n",
    "# whose keys are the current column names and whose keyvalues are the columns we want to replace them with.\n",
    "\n",
    "# The lines below until the next comment creates the dictionary to feed into .rename().\n",
    "dic2 = dict({})\n",
    "for x in trans_det_prod_roll.columns:\n",
    "    dic2.update({x:x})\n",
    "dic2.pop('netsalesamount')\n",
    "dic2.update({'netsalesamount':'to_netsalesamount'})\n",
    "# The line below actually does the renaming the columns of trans_det_prod_roll.\n",
    "# It .rename() creates a copy, so to override the current table, we must set the table\n",
    "# equal to the .rename() return copy.\n",
    "trans_det_prod_roll = trans_det_prod_roll.rename(index=str, columns=dic2)\n",
    "#####################\n",
    "\n",
    "\n",
    "\n",
    "# Start here 06/20/2019 !!!!!!!!!!!!!!!!!!\n",
    "# see datatypes of two tables in the merge statements below.\n",
    "# transactionkey types do not seem to match!!!!!!!!!!!!!!\n",
    "\n",
    "\n",
    "trans_product = pd.merge(trans_table, trans_det_prod_roll, how='inner', left_on = 'transactionkey', \\\n",
    "                        right_on = 'transactionkey')\n",
    "\n",
    "# Just doing some basic QA below\n",
    "print(\"active 12 month: \" , active12mo_table.agg({'customerkey':'count'}))\n",
    "print(\"trans_det: \" , transdet_table.agg({'transactionkey':'count'}))\n",
    "print(\"trans1det: \" , trans1det.agg({'transactionkey':'count'}))\n",
    "print(\"prod_table: \", prod_table.agg({'productkey':'count'}))\n",
    "print(\"trans_det_prod: \", trans_det_prod.agg({'transactionkey':'count'}))\n",
    "print(\"trans_det_prod_roll: \", trans_det_prod_roll.agg({'transactionkey':'count'}))\n",
    "print(\"trans_table: \", trans_table.agg({'transactionkey':'count'}))\n",
    "print(\"trans_product: \", trans_product.agg({'customerkey':'count'}))\n",
    "\n",
    "##########################\n",
    "\n",
    "\n",
    "\n",
    "trans_product = pd.merge(trans_product, trans_24mo, how = 'left', left_on = 'customerkey', right_on = 'customerkey')\n",
    "trans_product.agg({'customerkey':'count'})\n",
    "\n",
    "#print(trans_prod.query('activity_24mo_FL == 1'), trans_prod.agg({'customerkey':'count'}))\n",
    "\n",
    "print(trans_product.columns)\n",
    "\n",
    "lis2 = []\n",
    "for x in trans_product.columns:\n",
    "    if \"_FL\" in x:\n",
    "        trans_product[x] = trans_product[x].astype('int8')\n",
    "        lis2.append((x,'max'))\n",
    "        \n",
    "trans_product['transactiondate_min'] = trans_product['transactiondate']\n",
    "        \n",
    "dic2 = {'netsalesamount':'sum', \\\n",
    "                'transactiondate_min':'min', 'transactiondate':'max'}\n",
    "dic2.update(dict(lis2))\n",
    "        \n",
    "    \n",
    "trans_roll = trans_product.groupby('customerkey', as_index = False).agg(dic2)\n",
    "trans_roll_column_dict = dict({})\n",
    "\n",
    "\n",
    "for x in trans_roll.columns:\n",
    "    trans_roll_column_dict.update({x:x})\n",
    "trans_roll_column_dict.pop('transactiondate_min')\n",
    "trans_roll_column_dict.pop('transactiondate')\n",
    "\n",
    "\n",
    "dic3 = {'transactiondate_min':'first_visit', \\\n",
    "                    'transactiondate':'last_visit'}\n",
    "trans_roll_column_dict.update(dic3)\n",
    "\n",
    "trans_roll = trans_roll.rename(index = str, columns = dic3)\n",
    "\n",
    "mem_trans = pd.merge(mem_table, trans_roll, how = 'inner', left_on = 'customerkey', right_on = 'customerkey')\n",
    "\n",
    "# Basic QA below\n",
    "print(\"trans_roll: \", trans_roll.agg({'customerkey':'count'}))\n",
    "print(\"mem_trans: \", mem_trans.agg({'customerkey':'count'}))\n",
    "print(\"mem_table: \", mem_table.agg({'customerkey':'count'}))\n",
    "#####################\n",
    "\n",
    "\n",
    "mem_trans1 = mem_trans.copy()\n",
    "\n",
    "print(mem_trans1.dtypes)\n",
    "    \n",
    "\n",
    "mem_trans1['dsenroll'] = mem_trans['enrolldate'].apply(lambda x: (period_end - x.date()).days)\n",
    "mem_trans1['dslv'] = mem_trans['last_visit'].apply(lambda x: (period_end - x.date()).days)\n",
    "mem_trans1['store1distance'] = mem_trans1['store1distance'].fillna(-1)\n",
    "\n",
    "\n",
    "mem_trans1 = pd.merge(mem_trans1, active12mo_table, how = 'inner', on = 'customerkey')\n",
    "mem_trans1 = mem_trans1.loc[mem_trans1['trans_ct12mo'] <= 50]\n",
    "\n",
    "column_dict = dict({})\n",
    "\n",
    "\n",
    "for x in mem_trans1.columns:\n",
    "    column_dict.update({x:x})\n",
    "\n",
    "column_dict.pop('to_netsales12mo')\n",
    "column_dict.update({'to_netsales12mo':'totalnetspend12m'})\n",
    "column_dict\n",
    "\n",
    "mem_trans1 = mem_trans1.rename(columns = column_dict)\n",
    "print(mem_trans1.agg({'customerkey':'count'}))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#trans_det_prod.head(10)\n",
    "\n",
    "\n",
    "\n",
    "#sql_query_memtable.head(10)\n",
    "#df.to_csv('Q:\\\\Confidential\\\\DKNY\\\\Analytics\\\\2019\\\\201905 - Segmentation\\\\Eric\\\\Eric_csv_test.csv')\n",
    "# df.type([\"store1distance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customerkey    110801\n",
      "dtype: int64\n",
      "customerkey    170231\n",
      "dtype: int64\n",
      "customerkey    170231\n",
      "dtype: int64     customerkey enrolldate  store1distance  netsalesamount first_visit  \\\n",
      "0            10 2016-06-08         -1.0000           19.99  2018-09-07   \n",
      "1            11 2016-03-19         -1.0000          175.47  2018-10-14   \n",
      "2            23 2016-12-10         -1.0000          240.33  2018-06-15   \n",
      "3            28 2017-03-11         -1.0000          114.89  2018-11-22   \n",
      "4            31 2016-07-15         -1.0000          113.39  2018-07-22   \n",
      "5            43 2016-11-30         -1.0000           15.20  2018-06-19   \n",
      "6            56 2016-02-22         -1.0000           39.98  2018-07-15   \n",
      "7            76 2013-07-04         44.1857          236.97  2018-12-15   \n",
      "8           114 2015-06-04         -1.0000          158.60  2019-03-01   \n",
      "9           116 2015-05-20         -1.0000           64.97  2018-10-15   \n",
      "10          182 2013-06-25          6.2971          188.05  2018-06-23   \n",
      "11          189 2016-06-06         -1.0000          194.95  2018-07-04   \n",
      "12          208 2016-07-10         -1.0000          236.86  2019-02-10   \n",
      "13          220 2016-05-14         -1.0000          108.81  2018-12-15   \n",
      "14          221 2016-05-17         49.9697           21.75  2018-10-12   \n",
      "15          229 2016-11-15         -1.0000          366.52  2018-06-14   \n",
      "16          231 2016-06-26         -1.0000           29.98  2019-05-25   \n",
      "17          238 2016-06-11         -1.0000          424.60  2018-11-16   \n",
      "18          242 2016-06-25         -1.0000           29.99  2018-06-06   \n",
      "19          248 2016-11-19         -1.0000            9.93  2018-10-02   \n",
      "20        26409 2013-09-02         -1.0000           53.98  2018-08-18   \n",
      "21        36417 2013-09-20         -1.0000           19.98  2019-05-17   \n",
      "22        45735 2013-10-12         -1.0000          108.29  2018-10-05   \n",
      "23        45827 2013-11-07         -1.0000           59.97  2019-05-22   \n",
      "24        45837 2013-10-05         -1.0000           39.60  2018-11-30   \n",
      "25         9742 2013-08-07         -1.0000           29.50  2018-07-03   \n",
      "26         9777 2013-08-06         -1.0000          122.46  2018-07-05   \n",
      "27         9786 2013-08-06         -1.0000          214.17  2019-05-05   \n",
      "28        71922 2013-11-29         -1.0000           17.99  2019-04-27   \n",
      "29        72045 2014-02-08         -1.0000           19.99  2018-08-08   \n",
      "..          ...        ...             ...             ...         ...   \n",
      "70        55592 2013-12-23         -1.0000          160.46  2019-05-19   \n",
      "71        55645 2013-10-25         -1.0000           37.98  2018-07-01   \n",
      "72        55707 2013-11-21         -1.0000           40.08  2019-02-27   \n",
      "73          766 2013-07-27         -1.0000          599.40  2019-04-13   \n",
      "74          971 2013-07-30         -1.0000           17.99  2018-07-06   \n",
      "75        10323 2013-08-03         -1.0000           59.95  2018-11-18   \n",
      "76        36835 2013-09-28         -1.0000           60.00  2019-03-23   \n",
      "77        36849 2013-09-28         -1.0000           29.94  2018-06-02   \n",
      "78        36859 2013-09-28         23.3911          100.80  2019-02-25   \n",
      "79        55824 2013-12-05         -1.0000          219.16  2018-11-05   \n",
      "80        55870 2013-12-06         -1.0000           23.97  2019-04-28   \n",
      "81        55931 2013-10-22         -1.0000           29.98  2018-12-28   \n",
      "82        55952 2013-12-07          9.1385          457.65  2018-06-09   \n",
      "83        10339 2013-08-08         -1.0000          327.94  2019-02-15   \n",
      "84        10364 2013-08-05         38.4588           68.00  2018-08-15   \n",
      "85        10531 2013-08-08         -1.0000           35.40  2018-06-06   \n",
      "86         1010 2013-07-31         -1.0000          119.20  2019-02-18   \n",
      "87         1103 2013-08-01         -1.0000           60.00  2019-04-13   \n",
      "88         1109 2013-08-01         -1.0000           40.08  2018-09-12   \n",
      "89         1126 2013-08-01         -1.0000           39.99  2019-01-01   \n",
      "90         1134 2013-08-01          9.0762           71.39  2018-06-19   \n",
      "91        27115 2013-09-09         16.9367           29.25  2019-02-04   \n",
      "92        36968 2014-04-07         -1.0000           94.97  2018-09-03   \n",
      "93        46643 2013-10-27         -1.0000          600.09  2018-07-21   \n",
      "94        46707 2015-12-10         -1.0000           34.98  2018-11-16   \n",
      "95        46744 2013-10-14         -1.0000          175.03  2018-09-14   \n",
      "96        46745 2013-10-14         -1.0000           19.98  2018-10-13   \n",
      "97        46801 2013-10-12         -1.0000           39.97  2018-06-03   \n",
      "98        55996 2013-12-07         -1.0000           59.91  2018-10-13   \n",
      "99        56040 2013-12-07         -1.0000          387.10  2018-11-11   \n",
      "\n",
      "   last_visit  DK WOMENS APPAREL_FL  DK MENS APPAREL_FL  HANDBAGS_FL  \\\n",
      "0  2018-09-07                     1                   0            0   \n",
      "1  2018-10-14                     1                   1            0   \n",
      "2  2018-07-31                     1                   1            0   \n",
      "3  2018-11-22                     0                   1            0   \n",
      "4  2018-07-22                     0                   1            1   \n",
      "5  2018-06-19                     0                   0            0   \n",
      "6  2018-07-15                     1                   0            0   \n",
      "7  2019-05-05                     1                   0            0   \n",
      "8  2019-03-01                     1                   0            0   \n",
      "9  2018-10-15                     1                   0            1   \n",
      "10 2018-10-16                     1                   0            0   \n",
      "11 2019-02-09                     1                   1            0   \n",
      "12 2019-03-17                     1                   0            0   \n",
      "13 2018-12-17                     0                   0            0   \n",
      "14 2018-10-12                     0                   0            0   \n",
      "15 2019-05-15                     1                   1            0   \n",
      "16 2019-05-25                     0                   0            0   \n",
      "17 2019-04-29                     0                   0            0   \n",
      "18 2018-06-06                     0                   1            0   \n",
      "19 2018-10-02                     1                   0            0   \n",
      "20 2018-08-18                     0                   1            0   \n",
      "21 2019-05-17                     1                   0            0   \n",
      "22 2018-10-05                     1                   0            0   \n",
      "23 2019-05-22                     1                   0            0   \n",
      "24 2018-11-30                     0                   1            0   \n",
      "25 2018-07-03                     1                   0            0   \n",
      "26 2018-07-05                     1                   0            0   \n",
      "27 2019-05-05                     1                   1            0   \n",
      "28 2019-04-27                     0                   1            0   \n",
      "29 2018-08-08                     0                   1            0   \n",
      "..        ...                   ...                 ...          ...   \n",
      "70 2019-05-25                     1                   1            1   \n",
      "71 2018-08-05                     0                   1            0   \n",
      "72 2019-02-27                     0                   1            0   \n",
      "73 2019-04-13                     1                   1            0   \n",
      "74 2018-07-06                     1                   0            0   \n",
      "75 2018-11-18                     1                   0            0   \n",
      "76 2019-03-23                     0                   1            0   \n",
      "77 2018-06-02                     0                   1            0   \n",
      "78 2019-02-25                     0                   0            0   \n",
      "79 2018-11-05                     1                   0            0   \n",
      "80 2019-04-28                     1                   0            0   \n",
      "81 2018-12-28                     1                   0            0   \n",
      "82 2019-04-04                     0                   0            0   \n",
      "83 2019-02-15                     1                   1            0   \n",
      "84 2018-08-15                     0                   0            0   \n",
      "85 2018-06-06                     1                   0            0   \n",
      "86 2019-02-18                     0                   0            0   \n",
      "87 2019-04-13                     0                   1            0   \n",
      "88 2018-09-12                     1                   0            0   \n",
      "89 2019-01-01                     1                   0            0   \n",
      "90 2018-06-19                     1                   0            0   \n",
      "91 2019-02-04                     0                   0            0   \n",
      "92 2018-09-03                     0                   1            0   \n",
      "93 2019-03-16                     1                   1            1   \n",
      "94 2018-11-16                     0                   1            0   \n",
      "95 2019-03-12                     1                   1            0   \n",
      "96 2018-10-13                     1                   0            0   \n",
      "97 2018-06-03                     1                   0            0   \n",
      "98 2018-10-13                     1                   0            0   \n",
      "99 2019-05-03                     1                   1            1   \n",
      "\n",
      "    SMALL LEATHERGOODS_FL  ...  trans_ct12mo  totalnetspend12m  num_issued  \\\n",
      "0                       0  ...             1             19.99         1.0   \n",
      "1                       0  ...             1            175.47         2.0   \n",
      "2                       0  ...             3            240.33         2.0   \n",
      "3                       0  ...             1            114.89         1.0   \n",
      "4                       0  ...             1            113.39         1.0   \n",
      "5                       0  ...             1             15.20         1.0   \n",
      "6                       0  ...             1             39.98         1.0   \n",
      "7                       0  ...             2            236.97         1.0   \n",
      "8                       0  ...             1            158.60         0.0   \n",
      "9                       0  ...             1             64.97         1.0   \n",
      "10                      1  ...             2            188.05         2.0   \n",
      "11                      0  ...             3            194.95         3.0   \n",
      "12                      0  ...             2            236.86         1.0   \n",
      "13                      0  ...             2            108.81         1.0   \n",
      "14                      0  ...             1             21.75         1.0   \n",
      "15                      0  ...             8            366.52         5.0   \n",
      "16                      0  ...             1             29.98         0.0   \n",
      "17                      0  ...             3            424.60         2.0   \n",
      "18                      0  ...             1             29.99         0.0   \n",
      "19                      0  ...             1              9.93         0.0   \n",
      "20                      0  ...             1             53.98         1.0   \n",
      "21                      0  ...             1             19.98         0.0   \n",
      "22                      0  ...             1            108.29         2.0   \n",
      "23                      0  ...             1             59.97         0.0   \n",
      "24                      0  ...             1             39.60         0.0   \n",
      "25                      0  ...             1             29.50         0.0   \n",
      "26                      0  ...             1            122.46         1.0   \n",
      "27                      0  ...             1            214.17         0.0   \n",
      "28                      0  ...             1             17.99         0.0   \n",
      "29                      0  ...             1             19.99         2.0   \n",
      "..                    ...  ...           ...               ...         ...   \n",
      "70                      0  ...             3            160.46         0.0   \n",
      "71                      0  ...             2             37.98         0.0   \n",
      "72                      0  ...             1             40.08         1.0   \n",
      "73                      0  ...             1            599.40         1.0   \n",
      "74                      0  ...             1             17.99         1.0   \n",
      "75                      0  ...             1             59.95         1.0   \n",
      "76                      0  ...             1             60.00         0.0   \n",
      "77                      0  ...             1             29.94         1.0   \n",
      "78                      0  ...             1            100.80         1.0   \n",
      "79                      0  ...             1            219.16         1.0   \n",
      "80                      0  ...             1             23.97         0.0   \n",
      "81                      0  ...             1             29.98         0.0   \n",
      "82                      0  ...             8            457.65         4.0   \n",
      "83                      0  ...             1            327.94         1.0   \n",
      "84                      0  ...             1             68.00         1.0   \n",
      "85                      0  ...             1             35.40         0.0   \n",
      "86                      0  ...             1            119.20         1.0   \n",
      "87                      0  ...             1             60.00         0.0   \n",
      "88                      0  ...             1             40.08         1.0   \n",
      "89                      0  ...             1             39.99         0.0   \n",
      "90                      0  ...             2             71.39         1.0   \n",
      "91                      0  ...             1             29.25         0.0   \n",
      "92                      0  ...             1             94.97         0.0   \n",
      "93                      1  ...             8            600.09         4.0   \n",
      "94                      0  ...             1             34.98         0.0   \n",
      "95                      0  ...             4            175.03         1.0   \n",
      "96                      0  ...             1             19.98         1.0   \n",
      "97                      0  ...             1             39.97         0.0   \n",
      "98                      0  ...             1             59.91         1.0   \n",
      "99                      0  ...             3            387.10         1.0   \n",
      "\n",
      "    ever_redeemed_fl  last_reward_earned  first_reward_earned  red_fl  \\\n",
      "0                0.0          2018-10-12           2018-10-12       1   \n",
      "1                0.0          2018-11-15           2018-03-19       1   \n",
      "2                0.0          2018-11-15           2018-07-12       1   \n",
      "3                0.0          2018-12-14           2018-12-14       1   \n",
      "4                0.0          2018-11-15           2018-11-15       1   \n",
      "5                0.0          2018-07-12           2018-03-19       1   \n",
      "6                0.0          2018-11-15           2018-11-15       1   \n",
      "7                0.0          2019-01-11           2017-08-25       1   \n",
      "8                NaN          2019-04-12           2018-03-19       0   \n",
      "9                0.0          2018-11-15           2018-11-15       1   \n",
      "10               0.0          2018-11-15           2018-03-19       1   \n",
      "11               1.0          2018-11-15           2017-08-25       1   \n",
      "12               0.0          2019-03-14           2019-03-14       1   \n",
      "13               0.0          2019-01-11           2019-01-11       1   \n",
      "14               1.0          2018-07-12           2017-08-25       1   \n",
      "15               1.0          2019-03-14           2017-12-15       1   \n",
      "16               NaN          2019-06-01           2019-06-01       0   \n",
      "17               0.0          2019-05-14           2018-12-14       1   \n",
      "18               NaN          2018-01-12           2018-01-12       0   \n",
      "19               NaN          2018-03-19           2018-03-19       0   \n",
      "20               0.0          2018-11-15           2018-03-19       1   \n",
      "21               NaN          2018-03-19           2018-03-19       0   \n",
      "22               1.0          2018-07-12           2018-03-19       1   \n",
      "23               NaN          2018-03-19           2018-03-19       0   \n",
      "24               NaN          2018-03-19           2018-03-19       0   \n",
      "25               NaN          2019-06-01           2019-06-01       0   \n",
      "26               0.0          2018-08-15           2018-08-15       1   \n",
      "27               NaN          2018-03-19           2018-03-19       0   \n",
      "28               NaN          2018-03-19           2018-03-19       0   \n",
      "29               1.0          2018-07-12           2017-05-12       1   \n",
      "..               ...                 ...                  ...     ...   \n",
      "70               NaN          2018-03-19           2018-03-19       0   \n",
      "71               NaN          2018-03-19           2018-03-19       0   \n",
      "72               0.0          2019-03-14           2019-03-14       1   \n",
      "73               0.0          2019-05-14           2017-08-25       1   \n",
      "74               0.0          2018-11-15           2018-11-15       1   \n",
      "75               0.0          2018-07-12           2018-07-12       1   \n",
      "76               NaN          2019-04-12           2019-04-12       0   \n",
      "77               0.0          2018-07-12           2018-07-12       1   \n",
      "78               0.0          2019-03-14           2018-03-19       1   \n",
      "79               0.0          2018-12-14           2017-05-12       1   \n",
      "80               NaN          2018-03-19           2018-03-19       0   \n",
      "81               NaN          2019-06-01           2019-06-01       0   \n",
      "82               0.0          2019-01-11           2018-06-14       1   \n",
      "83               0.0          2019-04-12           2019-03-14       1   \n",
      "84               0.0          2018-09-14           2018-09-14       1   \n",
      "85               NaN          2019-06-01           2019-06-01       0   \n",
      "86               0.0          2019-03-14           2019-03-14       1   \n",
      "87               NaN          2018-03-19           2018-03-19       0   \n",
      "88               0.0          2018-10-12           2018-10-12       1   \n",
      "89               NaN          2019-04-12           2019-04-12       0   \n",
      "90               0.0          2018-07-12           2017-12-15       1   \n",
      "91               NaN          2019-06-01           2019-06-01       0   \n",
      "92               NaN          2018-03-19           2017-05-12       0   \n",
      "93               1.0          2019-04-12           2018-05-11       1   \n",
      "94               NaN          2019-06-01           2019-06-01       0   \n",
      "95               0.0          2019-04-12           2018-03-19       1   \n",
      "96               0.0          2018-11-15           2017-12-15       1   \n",
      "97               NaN          2017-08-25           2017-08-25       0   \n",
      "98               0.0          2018-04-13           2018-04-13       1   \n",
      "99               0.0          2018-12-14           2017-08-25       1   \n",
      "\n",
      "    earn_fl  dslreward  tenure  \n",
      "0         1        231     266  \n",
      "1         1        197     229  \n",
      "2         1        197     350  \n",
      "3         1        168     190  \n",
      "4         1        197     313  \n",
      "5         1        323     346  \n",
      "6         1        197     320  \n",
      "7         1        140     167  \n",
      "8         0         49      91  \n",
      "9         1        197     228  \n",
      "10        1        197     342  \n",
      "11        1        197     331  \n",
      "12        1         78     110  \n",
      "13        1        140     167  \n",
      "14        1        323     231  \n",
      "15        1         78     351  \n",
      "16        0         -1       6  \n",
      "17        1         17     196  \n",
      "18        0        504     359  \n",
      "19        0        438     241  \n",
      "20        1        197     286  \n",
      "21        0        438      14  \n",
      "22        1        323     238  \n",
      "23        0        438       9  \n",
      "24        0        438     182  \n",
      "25        0         -1     332  \n",
      "26        1        289     330  \n",
      "27        0        438      26  \n",
      "28        0        438      34  \n",
      "29        1        323     296  \n",
      "..      ...        ...     ...  \n",
      "70        0        438      12  \n",
      "71        0        438     334  \n",
      "72        1         78      93  \n",
      "73        1         17      48  \n",
      "74        1        197     329  \n",
      "75        1        323     194  \n",
      "76        0         49      69  \n",
      "77        1        323     363  \n",
      "78        1         78      95  \n",
      "79        1        168     207  \n",
      "80        0        438      33  \n",
      "81        0         -1     154  \n",
      "82        1        140     356  \n",
      "83        1         49     105  \n",
      "84        1        259     289  \n",
      "85        0         -1     359  \n",
      "86        1         78     102  \n",
      "87        0        438      48  \n",
      "88        1        231     261  \n",
      "89        0         49     150  \n",
      "90        1        323     346  \n",
      "91        0         -1     116  \n",
      "92        0        438     270  \n",
      "93        1         49     314  \n",
      "94        0         -1     196  \n",
      "95        1         49     259  \n",
      "96        1        197     230  \n",
      "97        0        644     362  \n",
      "98        1        413     230  \n",
      "99        1        168     201  \n",
      "\n",
      "[100 rows x 32 columns]\n",
      "customerkey    170231\n",
      "dtype: int64 Empty DataFrame\n",
      "Columns: [customerkey, store1distance, totalnetspend12m, trans_ct12mo, DK WOMENS APPAREL_FL, DK MENS APPAREL_FL, HANDBAGS_FL, SMALL LEATHERGOODS_FL, Womens Outerwear_FL, HATS / MISC_FL, Mens Outerwear_FL, FOOTWEAR_FL, APPAREL_FL, Outerwear_FL, WOMENS_FL, MENS_FL, ECOMMERCE_FL, activity_24mo_FL, dsenroll, dslv, num_issued, ever_redeemed_fl, earn_fl, dslreward, tenure, segment]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 26 columns]\n",
      "Empty DataFrame\n",
      "Columns: [customerkey, store1distance, totalnetspend12m, trans_ct12mo, DK WOMENS APPAREL_FL, DK MENS APPAREL_FL, HANDBAGS_FL, SMALL LEATHERGOODS_FL, Womens Outerwear_FL, HATS / MISC_FL, Mens Outerwear_FL, FOOTWEAR_FL, APPAREL_FL, Outerwear_FL, WOMENS_FL, MENS_FL, ECOMMERCE_FL, activity_24mo_FL, dsenroll, dslv, num_issued, ever_redeemed_fl, earn_fl, dslreward, tenure, segment]\n",
      "Index: []\n",
      "\n",
      "[0 rows x 26 columns]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# SQL query for a 12 Month Issuance Table\n",
    "iss_query = \"\"\" \\\n",
    "select customerkey, rewardnumber, loyaltyrewardkey, rewardamount, \\\n",
    "rewardsweepkey, rewarddate, expirationdate, rewardamountused\n",
    "from fact_loyaltyreward a,\n",
    "    fact_membership b\n",
    "    where a.customermembershipkey = b.customermembershipkey\n",
    "    and rewardsweepkey between \"\"\" + str(sweepkey - 13) + \" and \" + str(sweepkey - 2)\n",
    "\n",
    "# Creation of 12 Month Issuance Table\n",
    "iss_table = pd.read_sql_query(iss_query, connection)\n",
    "\n",
    "# SQL query for Redemption Table in Sweep of Interest\n",
    "red_query = \"\"\" \\\n",
    "select loyaltyrewardkey, transactionamount, 1 as red_fl\n",
    "from fact_loyaltyreward a,\n",
    "    fact_rewardredemption b\n",
    "    where a.loyaltyrewardkey = b.loyaltyrewardid\n",
    "    and convert(date,b.transactiondtm) <= \"\"\" + \"'\" + dt_form(period_end) + \"'\"\n",
    "\n",
    "# Creation of Redemption table\n",
    "red_table = pd.read_sql_query(red_query, connection)\n",
    "\n",
    "#QA Check\n",
    "#print(iss_table.agg({'customerkey':'count'}), red_table.agg({'loyaltyrewardkey':'count'}))\n",
    "\n",
    "\n",
    "iss_red = pd.merge(iss_table, red_table, how = 'left', on = 'loyaltyrewardkey')\n",
    "\n",
    "#Setting red_fl to 0 for customers that have been issued a reward\n",
    "# but have never redeemed a reward.\n",
    "iss_red['red_fl'] = iss_red['red_fl'].fillna(0.0)\n",
    "\n",
    "#QA check\n",
    "#iss_red.query('red_fl != red_fl')\n",
    "\n",
    "\n",
    "iss_red_roll = iss_red.groupby('customerkey', as_index = False).agg({'rewardnumber':'count', 'red_fl':'max'})\n",
    "iss_red_roll = iss_red_roll.rename(columns = {'customerkey':'customerkey', 'rewardnumber':'num_issued', \\\n",
    "                                    'red_fl':'ever_redeemed_fl'})\n",
    "\n",
    "\n",
    "print(iss_red_roll.agg({'customerkey':'count'}))\n",
    "\n",
    "# Create Last Issuance Table\n",
    "\n",
    "last_iss_query = \"\"\"\\\n",
    "select customerkey, max(convert(date,rewarddate)) as last_reward_earned, \\\n",
    "min(convert(date,rewarddate)) as first_reward_earned\n",
    "from fact_loyaltyreward a,\n",
    "    fact_membership b\n",
    "    where a.customermembershipkey = b.customermembershipkey\n",
    "    and convert(date,rewarddate) <= \"\"\" + \"'\" + dt_form(period_end) + \"'\" + \"\"\" \\\n",
    "    group by customerkey\n",
    "    order by customerkey; \"\"\"\n",
    "\n",
    "last_iss = pd.read_sql_query(last_iss_query, connection)\n",
    "#print(last_iss.head(100), last_iss.agg({'customerkey':'count'}))\n",
    "\n",
    "all_table = pd.merge(mem_trans1, iss_red_roll, how = 'left', on = 'customerkey')\n",
    "print(all_table.agg({'customerkey':'count'}))\n",
    "\n",
    "\n",
    "all_table = pd.merge(all_table, last_iss, how = 'left', on = 'customerkey')\n",
    "\n",
    "\n",
    "all_table['last_reward_earned'] = all_table['last_reward_earned'].fillna(value = dt_form(period_end + one_day))\n",
    "all_table['first_reward_earned'] = all_table['first_reward_earned'].fillna(value = dt_form(period_end + one_day))\n",
    "all_table['red_fl'] = all_table['ever_redeemed_fl']\n",
    "all_table['red_fl'] = np.where(all_table['red_fl'] != all_table['red_fl'], 0, 1)\n",
    "all_table['num_issued'] = all_table['num_issued'].fillna(0.0)\n",
    "all_table['earn_fl'] = np.where(all_table['num_issued'] >= 1, 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "all_table['dslreward'] = all_table['last_reward_earned'].apply(lambda x: \\\n",
    "                                    (period_end - dt.datetime.strptime(str(x), '%Y-%m-%d').date()).days)\n",
    "all_table['tenure'] = all_table['first_visit'].apply(lambda x: \\\n",
    "                                    (period_end - x.date()).days)\n",
    "print(all_table.agg({'customerkey':'count'}), all_table.head(100))\n",
    "\n",
    "\n",
    "all_table['ever_redeemed_fl'] = all_table['ever_redeemed_fl'].fillna(0.0)\n",
    "\n",
    "all_table2 = all_table[['customerkey', 'store1distance', 'totalnetspend12m', 'trans_ct12mo', \\\n",
    "'DK WOMENS APPAREL_FL', 'DK MENS APPAREL_FL', 'HANDBAGS_FL', 'SMALL LEATHERGOODS_FL', 'Womens Outerwear_FL', \\\n",
    "'HATS / MISC_FL', 'Mens Outerwear_FL', 'FOOTWEAR_FL', 'APPAREL_FL', 'Outerwear_FL', 'WOMENS_FL', \\\n",
    "'MENS_FL', 'ECOMMERCE_FL', 'activity_24mo_FL', 'dsenroll', 'dslv', 'num_issued', 'ever_redeemed_fl', \\\n",
    "'earn_fl', 'dslreward', 'tenure']]\n",
    "\n",
    "\n",
    "\n",
    "########### Creation of the Final Segmentation Table\n",
    "segmentation = all_table2.copy()\n",
    "\n",
    "\n",
    "segmentation.dtypes\n",
    "\n",
    "conditions = [\\\n",
    "    (segmentation['trans_ct12mo'] > 3), \\\n",
    "    (segmentation['trans_ct12mo'] == 2) | (segmentation['trans_ct12mo'] == 3), \\\n",
    "    (segmentation['trans_ct12mo'] == 1) & (segmentation['dsenroll'] <= 90), \\\n",
    "    (segmentation['trans_ct12mo'] == 1) & (segmentation['dsenroll'] > 90) & \\\n",
    "              (segmentation['dslv'] <= 270), \\\n",
    "    (segmentation['trans_ct12mo'] == 1) & (segmentation['dsenroll'] > 90) & \\\n",
    "              (segmentation['dslv'] > 270)]\n",
    "\n",
    "choices = [1,2,3,4,5]\n",
    "segmentation['segment'] = np.select(conditions, choices, default='uh-oh!')\n",
    "\n",
    "\n",
    "\n",
    "print(segmentation.agg({'customerkey': 'count'}), segmentation.query(\"segment == 'uh-oh!'\"))\n",
    "print(segmentation.query('trans_ct12mo == 0'))\n",
    "#segmentation.to_excel('Q:\\\\Confidential\\\\Analytics\\\\Interns 2019\\\\Eric\\\\DKNY Segmentation Build.xlsx')\n",
    "\n",
    "\n",
    "#segall = pd.read_csv('Q:\\\\Confidential\\\\DKNY\\\\Analytics\\\\2019\\\\201905 - Segmentation\\\\Eric\\\\segall.csv')\n",
    "#print(segall.agg({'CustomerKey':'count'}))\n",
    "\n",
    "######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        customerkey segment   0  transactiondate\n",
      "2                23       2 NaN             46.0\n",
      "8                76       2 NaN            141.0\n",
      "11              182       2 NaN            115.0\n",
      "12              189       2 NaN            164.0\n",
      "13              189       2 NaN             56.0\n",
      "14              208       2 NaN             35.0\n",
      "15              220       2 NaN              2.0\n",
      "17              229       1 NaN             20.0\n",
      "18              229       1 NaN             14.0\n",
      "19              229       1 NaN             14.0\n",
      "20              229       1 NaN            189.0\n",
      "21              229       1 NaN             97.0\n",
      "22              229       1 NaN              1.0\n",
      "25              238       2 NaN             79.0\n",
      "26              238       2 NaN             85.0\n",
      "29              269       2 NaN             56.0\n",
      "37              319       2 NaN            185.0\n",
      "39              327       1 NaN              9.0\n",
      "40              327       1 NaN              4.0\n",
      "41              327       1 NaN            110.0\n",
      "42              327       1 NaN             15.0\n",
      "44              342       2 NaN             29.0\n",
      "47              355       2 NaN             27.0\n",
      "48              393       1 NaN              8.0\n",
      "49              393       1 NaN              1.0\n",
      "50              393       1 NaN             27.0\n",
      "51              399       2 NaN             28.0\n",
      "52              409       2 NaN             19.0\n",
      "65             1633       2 NaN            121.0\n",
      "67             1658       2 NaN            139.0\n",
      "...             ...     ...  ..              ...\n",
      "210105      2288147       2 NaN              3.0\n",
      "210110      2288152       2 NaN              1.0\n",
      "210129      2288171       2 NaN              4.0\n",
      "210176      2288218       2 NaN              2.0\n",
      "210210      2288252       2 NaN              2.0\n",
      "210375      2288417       2 NaN              1.0\n",
      "210395      2288436       2 NaN              1.0\n",
      "210428      2288468       2 NaN              2.0\n",
      "210435      2288475       2 NaN              1.0\n",
      "210448      2288487       2 NaN              2.0\n",
      "210452      2288491       2 NaN              2.0\n",
      "210453      2288491       2 NaN              1.0\n",
      "210469      2288507       2 NaN              1.0\n",
      "210522      2288558       2 NaN              1.0\n",
      "210555      2288591       2 NaN              3.0\n",
      "210805      2288842       2 NaN              2.0\n",
      "210819      2288854       2 NaN              2.0\n",
      "210844      2288878       2 NaN              1.0\n",
      "210930      2288963       2 NaN              1.0\n",
      "210931      2288964       2 NaN              1.0\n",
      "210952      2288985       2 NaN              1.0\n",
      "210993      2289023       2 NaN              2.0\n",
      "211019      2289048       2 NaN              1.0\n",
      "211036      2289065       2 NaN              1.0\n",
      "211189      2289220       2 NaN              4.0\n",
      "211211      2289242       2 NaN              9.0\n",
      "211494      2289525       2 NaN              1.0\n",
      "211539      2289568       2 NaN              1.0\n",
      "211596      2289623       2 NaN              1.0\n",
      "212357      2292796       2 NaN              1.0\n",
      "\n",
      "[60704 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# Creation of a Days Between Visits Table for Customers on Segmentation Table\n",
    "seg_custs = segmentation[['customerkey', 'segment']]\n",
    "trans_12mo = trans_table.loc[trans_table['transactiondate'] >= pd.Timestamp(period_12mo + one_day)]\n",
    "seg_dbtwv = pd.merge(seg_custs, trans_12mo, how = 'left', on = 'customerkey')\n",
    "dbtwv1 = seg_dbtwv.copy()\n",
    "\n",
    "\n",
    "# Creation of Smaller Dataset for QA pruposes.\n",
    "#temp2 = temp2.iloc[0:20000]\n",
    "########################\n",
    "\n",
    "\n",
    "def dbtwv_calc(df):\n",
    "    if len(df['transactiondate']) == 1:\n",
    "        df.at[:,'transactiondate'] = -1\n",
    "        return df['transactiondate']\n",
    "    \n",
    "    else:\n",
    "        df['transactiondate'] = df['transactiondate'].sort_values()\n",
    "        diff = df['transactiondate'].diff()\n",
    "        diff = pd.DataFrame({'transactiondate':diff})\n",
    "        diff = diff.dropna()\n",
    "        diff['transactiondate'] = diff['transactiondate'].apply(lambda x: x.days)\n",
    "        \n",
    "        return diff\n",
    "\n",
    "\n",
    "dbtwv1 = dbtwv1.sort_values('transactiondate')    \n",
    "dbtwv1 = dbtwv1.groupby(['customerkey','segment']).apply(dbtwv_calc)\n",
    "dbtwv1 = dbtwv1.reset_index()\n",
    "\n",
    "dbtwv = pd.DataFrame(dbtwv1)\n",
    "dbtwv = dbtwv.drop('level_2', axis = 1)\n",
    "dbtwv = dbtwv[dbtwv.transactiondate != -1]\n",
    "dbtwv = dbtwv[dbtwv.transactiondate > 0]\n",
    "print(dbtwv)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['customerkey', 'store1distance', 'totalnetspend12m', 'trans_ct12mo', 'DK WOMENS APPAREL_FL', 'DK MENS APPAREL_FL', 'HANDBAGS_FL', 'SMALL LEATHERGOODS_FL', 'Womens Outerwear_FL', 'HATS / MISC_FL', 'Mens Outerwear_FL', 'FOOTWEAR_FL', 'APPAREL_FL', 'Outerwear_FL', 'WOMENS_FL', 'MENS_FL', 'ECOMMERCE_FL', 'activity_24mo_FL', 'dsenroll', 'dslv', 'num_issued', 'ever_redeemed_fl', 'earn_fl', 'dslreward', 'tenure', 'segment']\n"
     ]
    }
   ],
   "source": [
    "# Creation of Segment Level Statistics\n",
    "\n",
    "print(segmentation.columns.to_list())\n",
    "seg_stats = segmentation[['store1distance', 'totalnetspend12m', 'trans_ct12mo', \\\n",
    "                         'DK WOMENS APPAREL_FL', 'DK MENS APPAREL_FL', 'HANDBAGS_FL', 'SMALL LEATHERGOODS_FL', \\\n",
    "                         'Womens Outerwear_FL', 'HATS / MISC_FL', 'Mens Outerwear_FL', 'FOOTWEAR_FL', 'APPAREL_FL', \\\n",
    "                         'Outerwear_FL', 'WOMENS_FL', 'MENS_FL', 'ECOMMERCE_FL', 'activity_24mo_FL', 'dsenroll', \\\n",
    "                         'dslv', 'num_issued', 'ever_redeemed_fl', 'earn_fl', 'dslreward', 'tenure', \\\n",
    "                         'segment']]\n",
    "seg_stats = seg_stats.loc[seg_stats['store1distance'] >= 0.0]\n",
    "seg_stats = seg_stats.groupby('segment')\n",
    "seg_stats = seg_stats.agg([len, np.size, np.mean, np.std, np.min, np.max])\n",
    "seg_stats = seg_stats.T\n",
    "#seg_stats.to_excel('Q:\\\\Confidential\\\\Analytics\\\\Interns 2019\\\\Eric\\\\DKNY\\\\DKNY Segmentation Statistics.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "customerkey                       int64\n",
      "enrolldate               datetime64[ns]\n",
      "store1distance                  float64\n",
      "netsalesamount                  float64\n",
      "first_visit              datetime64[ns]\n",
      "last_visit               datetime64[ns]\n",
      "DK WOMENS APPAREL_FL               int8\n",
      "DK MENS APPAREL_FL                 int8\n",
      "HANDBAGS_FL                        int8\n",
      "SMALL LEATHERGOODS_FL              int8\n",
      "Womens Outerwear_FL                int8\n",
      "HATS / MISC_FL                     int8\n",
      "Mens Outerwear_FL                  int8\n",
      "FOOTWEAR_FL                        int8\n",
      "APPAREL_FL                         int8\n",
      "Outerwear_FL                       int8\n",
      "WOMENS_FL                          int8\n",
      "MENS_FL                            int8\n",
      "ECOMMERCE_FL                       int8\n",
      "activity_24mo_FL                   int8\n",
      "dsenroll                          int64\n",
      "dslv                              int64\n",
      "trans_ct12mo                      int64\n",
      "totalnetspend12m                float64\n",
      "num_issued                      float64\n",
      "ever_redeemed_fl                float64\n",
      "last_reward_earned               object\n",
      "first_reward_earned              object\n",
      "red_fl                            int32\n",
      "earn_fl                           int32\n",
      "dtype: object customerkey                     int64\n",
      "last_reward_earned     datetime64[ns]\n",
      "first_reward_earned    datetime64[ns]\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
